#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.10"
# dependencies = []
# ///

# ruff: noqa: E402
# Auto-generated by build.py - Do not edit directly
# Edit source files in src/ and run build.py to regenerate

# === partition.py ===
import zlib
from collections import OrderedDict
from pathlib import Path

# 1MB buffer for efficient I/O
BUFFER_SIZE = 1024 * 1024

# Maximum number of file handles to keep open at once (LRU cache limit)
MAX_OPEN_HANDLES = 128


class LRUFileCache:
    """
    LRU cache for file handles to prevent file descriptor exhaustion.

    Keeps at most MAX_OPEN_HANDLES files open, evicting least-recently-used
    handles when the limit is exceeded.
    """

    def __init__(self, max_handles: int, tmp_dir: Path):
        self._max_handles = max_handles
        self._tmp_dir = tmp_dir
        self._cache: OrderedDict[int, object] = OrderedDict()

    def _get_path(self, bucket_idx: int) -> Path:
        return self._tmp_dir / f"bucket_{bucket_idx:04d}.bin"

    def write(self, bucket_idx: int, data: bytes) -> None:
        """Write data to the specified bucket, opening handle if needed."""
        if bucket_idx in self._cache:
            # Move to end (most recently used)
            self._cache.move_to_end(bucket_idx)
            handle = self._cache[bucket_idx]
        else:
            # Evict LRU if at capacity
            while len(self._cache) >= self._max_handles:
                _, old_handle = self._cache.popitem(last=False)
                old_handle.close()

            # Open new handle
            handle = open(self._get_path(bucket_idx), "ab", buffering=BUFFER_SIZE)
            self._cache[bucket_idx] = handle

        handle.write(data)

    def close_all(self) -> None:
        """Close all open file handles."""
        for handle in self._cache.values():
            handle.close()
        self._cache.clear()


def partition_to_buckets(
    input_path: str,
    num_buckets: int,
    tmp_dir: str,
) -> list[Path]:
    """
    Partition input file into buckets based on (claim_id, status_code) hash.

    Uses an LRU cache to limit the number of open file handles.

    Args:
        input_path: Path to the input file (pipe-delimited).
        num_buckets: Number of buckets (should be power of 2 for fast modulo).
        tmp_dir: Directory to store bucket files.

    Returns:
        List of paths to non-empty bucket files.
    """
    tmp_path = Path(tmp_dir)
    tmp_path.mkdir(parents=True, exist_ok=True)

    bucket_mask = num_buckets - 1
    cache = LRUFileCache(MAX_OPEN_HANDLES, tmp_path)

    try:
        with open(input_path, "rb") as f:
            for line in f:
                line = line.rstrip(b"\n\r")
                if not line:
                    continue

                # Parse with maxsplit=3 for speed
                parts = line.split(b"|", 3)
                if len(parts) < 4:
                    continue

                claim_bytes = parts[2]
                status_bytes = parts[3]

                # Stable hash for bucket assignment
                bucket_idx = zlib.crc32(claim_bytes + b"|" + status_bytes) & bucket_mask

                # Write raw line bytes with newline
                cache.write(bucket_idx, line + b"\n")

    finally:
        cache.close_all()

    # Return only non-empty bucket files
    non_empty = [
        tmp_path / f"bucket_{i:04d}.bin"
        for i in range(num_buckets)
        if (tmp_path / f"bucket_{i:04d}.bin").exists()
        and (tmp_path / f"bucket_{i:04d}.bin").stat().st_size > 0
    ]
    return non_empty


# === graph.py ===
from collections import defaultdict


def process_bucket(bucket_path: str) -> tuple[bytes, bytes, int] | None:
    """
    Process a single bucket file to find the longest cycle.

    Builds adjacency sets grouped by (claim_id, status_code) and finds
    the longest simple cycle within each group. All processing uses bytes
    until final result.

    Args:
        bucket_path: Path to the bucket file.

    Returns:
        Tuple of (claim_id, status_code, cycle_length) as bytes, or None if no cycles.
    """
    # Build adjacency sets: edges[(claim, status)][source] -> set(destinations)
    # Also track if each group is a functional graph (max out-degree <= 1)
    edges: dict[tuple[bytes, bytes], dict[bytes, set[bytes]]] = defaultdict(
        lambda: defaultdict(set)
    )
    # Track max out-degree per group to detect functional graphs
    max_out_degree: dict[tuple[bytes, bytes], int] = defaultdict(int)

    # Read and parse bucket file - keep everything as bytes
    with open(bucket_path, "rb") as f:
        for line in f:
            line = line.rstrip(b"\n\r")
            if not line:
                continue

            parts = line.split(b"|", 3)
            if len(parts) < 4:
                continue

            source = parts[0]
            dest = parts[1]
            claim_id = parts[2]
            status = parts[3]

            key = (claim_id, status)
            adj = edges[key]

            # Add edge (deduplicates via set)
            old_size = len(adj[source])
            adj[source].add(dest)
            new_size = len(adj[source])

            # Update max out-degree if this edge was new
            if new_size > old_size:
                if new_size > max_out_degree[key]:
                    max_out_degree[key] = new_size

    # Find longest cycle across all (claim_id, status) groups
    best_result: tuple[bytes, bytes, int] | None = None

    for key, adj in edges.items():
        is_functional = max_out_degree[key] <= 1
        cycle_len = _find_longest_cycle(adj, is_functional)
        if cycle_len > 0:
            if best_result is None or cycle_len > best_result[2]:
                best_result = (key[0], key[1], cycle_len)

    return best_result


def _find_longest_cycle(adj: dict[bytes, set[bytes]], is_functional: bool) -> int:
    """
    Find the longest simple cycle in a directed graph.

    Args:
        adj: Adjacency sets mapping source -> set(destinations).
        is_functional: True if max out-degree <= 1 (use O(N) algorithm).

    Returns:
        Length of the longest simple cycle, or 0 if none found.
    """
    if not adj:
        return 0

    if is_functional:
        return _find_cycle_functional(adj)
    else:
        return _find_cycle_dfs(adj)


def _find_cycle_functional(adj: dict[bytes, set[bytes]]) -> int:
    """
    O(N) cycle detection for functional graphs (out-degree <= 1).

    Walks each path and detects cycles when revisiting a node in the current path.
    """
    # Build a simple next-node mapping
    next_node: dict[bytes, bytes] = {}
    for src, dests in adj.items():
        if dests:
            next_node[src] = next(iter(dests))

    if not next_node:
        return 0

    # Track globally visited nodes (already processed)
    globally_visited: set[bytes] = set()
    longest = 0

    # Also consider nodes that are only destinations (might be part of cycles)
    all_nodes = set(next_node.keys())
    for dest in next_node.values():
        all_nodes.add(dest)

    for start in all_nodes:
        if start in globally_visited:
            continue

        # Walk the path, tracking nodes in current walk with their positions
        path_order: dict[bytes, int] = {}
        current = start
        pos = 0

        while current is not None:
            if current in globally_visited:
                break

            if current in path_order:
                # Found a cycle
                cycle_len = pos - path_order[current]
                if cycle_len >= 2 and cycle_len > longest:
                    longest = cycle_len
                break

            path_order[current] = pos
            pos += 1
            current = next_node.get(current)

        globally_visited.update(path_order.keys())

    return longest


def _find_cycle_dfs(adj: dict[bytes, set[bytes]]) -> int:
    """
    DFS-based cycle detection for general graphs.

    Uses minimum start node rule to avoid counting the same cycle multiple times:
    - Sort all nodes
    - When starting DFS from node at index i, only explore neighbors with index >= i
    - This ensures each cycle is found exactly once (from its minimum node)
    """
    # Get all nodes that have outgoing edges
    nodes_with_edges = list(adj.keys())
    if not nodes_with_edges:
        return 0

    # Sort nodes for consistent ordering
    nodes_sorted = sorted(nodes_with_edges)
    idx_map: dict[bytes, int] = {node: i for i, node in enumerate(nodes_sorted)}

    longest = 0

    def dfs(node: bytes, start: bytes, start_idx: int, path: set[bytes], depth: int) -> None:
        """DFS with backtracking, only exploring nodes with index >= start_idx."""
        nonlocal longest

        for neighbor in adj.get(node, set()):
            if neighbor == start and depth >= 1:
                # Found a cycle back to start (length = depth + 1)
                cycle_len = depth + 1
                if cycle_len > longest:
                    longest = cycle_len
            elif neighbor not in path:
                # Only continue to nodes with index >= start_idx
                neighbor_idx = idx_map.get(neighbor)
                if neighbor_idx is not None and neighbor_idx >= start_idx:
                    path.add(neighbor)
                    dfs(neighbor, start, start_idx, path, depth + 1)
                    path.remove(neighbor)

    # Start DFS from each node
    for i, start_node in enumerate(nodes_sorted):
        path = {start_node}
        dfs(start_node, start_node, i, path, 0)

    return longest


# === scheduler.py ===
import logging
import os
import shutil
import sys
import tempfile
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from pathlib import Path


logger = logging.getLogger(__name__)

# Each process recieves 16 buckets to process
PROCESS_POOL_CHUNKSIZE = 16

# Environment variable to override executor selection
RC_EXECUTOR_ENV = "RC_EXECUTOR"


def _is_gil_enabled() -> bool:
    """Check if GIL is enabled."""
    try:
        return sys._is_gil_enabled()
    except AttributeError:
        return True


def _get_executor_class():
    """
    Select the appropriate executor.

    Priority:
    1. RC_EXECUTOR env var override ("threads" or "processes")
    2. Auto-select based on GIL status (disabled -> threads, enabled -> processes)
    """
    executor_override = os.environ.get(RC_EXECUTOR_ENV, "").lower()

    if executor_override == "threads":
        return ThreadPoolExecutor
    elif executor_override == "processes":
        return ProcessPoolExecutor
    else:
        if _is_gil_enabled():
            return ProcessPoolExecutor
        else:
            return ThreadPoolExecutor


def solve(
    input_path: str,
    buckets: int = 1024,
    workers: int | None = None,
) -> tuple[str, str, int] | None:
    """
    Find the longest cycle in the routing data.

    Two-pass algorithm:
    1. Partition data into buckets by (claim_id, status_code) hash
    2. Process each bucket in parallel to find cycles
    3. Reduce to find the global maximum

    Args:
        input_path: Path to the input file.
        buckets: Number of buckets for partitioning (power of 2).
        workers: Number of parallel workers (None = auto).

    Returns:
        Tuple of (claim_id, status_code, cycle_length) or None if no cycles.
    """
    input_path = str(Path(input_path).resolve())

    # Validate buckets is power of 2
    if buckets & (buckets - 1) != 0:
        raise ValueError(f"buckets must be a power of 2, got {buckets}")

    # Select executor based on GIL status
    ExecutorClass = _get_executor_class()
    use_process_pool = ExecutorClass is ProcessPoolExecutor

    gil_status = "enabled" if _is_gil_enabled() else "disabled"
    logger.debug("GIL status: %s", gil_status)
    logger.debug("Using executor: %s", ExecutorClass.__name__)

    # Create temporary directory for buckets
    tmp_dir = tempfile.mkdtemp(prefix="routing_cycles_")

    try:
        # Pass 1: Partition to buckets
        logger.debug("Pass 1: Partitioning to %d buckets...", buckets)

        bucket_paths = partition_to_buckets(input_path, buckets, tmp_dir)

        logger.debug("  Created %d non-empty buckets", len(bucket_paths))

        if not bucket_paths:
            return None

        # Pass 2: Process buckets in parallel using executor.map
        logger.debug("Pass 2: Processing buckets...")

        # Convert Path objects to strings for pickling (ProcessPoolExecutor)
        bucket_path_strs = [str(p) for p in bucket_paths]

        # Best result: (claim_id_bytes, status_bytes, cycle_len)
        best_result: tuple[bytes, bytes, int] | None = None

        with ExecutorClass(max_workers=workers) as executor:
            if use_process_pool:
                # Use chunksize to reduce IPC overhead
                results = executor.map(
                    process_bucket,
                    bucket_path_strs,
                    chunksize=PROCESS_POOL_CHUNKSIZE,
                )
            else:
                # ThreadPoolExecutor - chunksize not as critical
                results = executor.map(process_bucket, bucket_path_strs)

            for result in results:
                if result is not None:
                    if best_result is None or result[2] > best_result[2]:
                        best_result = result
                        claim_id = result[0].decode("utf-8")
                        status = result[1].decode("utf-8")
                        logger.debug("  New best: %s,%s,%d", claim_id, status, result[2])

        logger.debug("Pass 2: Complete.")

        # Decode bytes to strings for final result
        if best_result is not None:
            return (
                best_result[0].decode("utf-8"),
                best_result[1].decode("utf-8"),
                best_result[2],
            )
        return None

    finally:
        # Cleanup temporary directory
        shutil.rmtree(tmp_dir, ignore_errors=True)


def main_solve(input_path: str, buckets: int = 1024) -> None:
    """
    Main entry point that prints result to stdout.

    Args:
        input_path: Path to the input file.
        buckets: Number of buckets for partitioning.
    """
    result = solve(input_path, buckets=buckets)

    if result:
        claim_id, status_code, cycle_length = result
        print(f"{claim_id},{status_code},{cycle_length}")
    else:
        print(0)


# === main.py ===
import argparse
import logging
import sys



def configure_logging(level: int = logging.WARNING) -> None:
    """Configure logging to write to stderr."""
    logging.basicConfig(
        level=level,
        format="%(levelname)s: %(message)s",
        stream=sys.stderr,
    )


def create_parser() -> argparse.ArgumentParser:
    """Create and configure the argument parser."""
    parser = argparse.ArgumentParser(
        prog="routing-cycle-detector",
        description="Find the longest routing cycle in claim data.",
    )

    parser.add_argument(
        "input_file",
        help="Path to the input file (pipe-delimited: Source|Dest|ClaimID|Status)",
    )

    parser.add_argument(
        "--buckets",
        type=int,
        default=1024,
        help="Number of buckets for partitioning (power of 2, default: 1024)",
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable verbose logging (DEBUG level)",
    )

    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging (same as --verbose)",
    )

    return parser


def main() -> None:
    """Main entry point."""
    parser = create_parser()
    args = parser.parse_args()

    # Configure logging based on verbosity
    log_level = logging.DEBUG if (args.verbose or args.debug) else logging.WARNING
    configure_logging(log_level)

    # Validate buckets is power of 2
    if args.buckets & (args.buckets - 1) != 0:
        parser.error(f"--buckets must be a power of 2, got {args.buckets}")

    main_solve(
        input_path=args.input_file,
        buckets=args.buckets,
    )


if __name__ == "__main__":
    main()

